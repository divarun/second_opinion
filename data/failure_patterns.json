[
  {
    "id": "fanout_retry_amplification",
    "name": "Fanout + Retry Amplification",
    "signals": ["fanout", "retry", "dependency"],
    "required_context": ["fanout_factor", "retry_policy"],
    "trigger_conditions": [
      "fanout_factor > 5",
      "static_retry_count > 1",
      "no_deadline_propagation"
    ],
    "safety_signals": [
      "retry_budget",
      "request_hedging",
      "circuit_breaker",
      "adaptive_concurrency_limit"
    ],
    "why_subtle": [
      "p50 latency unaffected",
      "staging does not simulate partial failure"
    ],
    "discussion_questions": [
      "Is there end-to-end deadline propagation?",
      "Is there a global retry budget (e.g., <10% of total traffic)?",
      "What happens if one leaf node in the fanout is permanently slow?"
    ]
  },
  {
    "id": "queue_backpressure",
    "name": "Queue Buildup / Backpressure Cascade",
    "signals": ["queue", "latency", "throughput"],
    "required_context": ["queue_size_limit", "consumer_throughput", "producer_rate"],
    "trigger_conditions": [
      "consumer_throughput < producer_rate",
      "queue_size > 80% capacity",
      "no_backpressure_signaling",
      "dependencies under partial degradation"
    ],
    "safety_signals": [
      "backpressure_propagation",
      "consumer_autoscaling",
      "queue_lag_monitoring",
      "circuit_breaker_on_lag"
    ],
    "why_subtle": [
      "p50 latency may remain normal",
      "emerges only under sustained high load",
      "staging often has smaller data volumes"
    ],
    "discussion_questions": [
      "Are consumers scaling with producer throughput?",
      "Are queues monitored with lag alerts?",
      "Is backpressure handled gracefully upstream?",
      "What happens when queue reaches capacity?"
    ]
  },
  {
    "id": "cache_stampede",
    "name": "Cache Stampede / Hot Key Overload",
    "signals": ["cache", "hot_key", "latency_spike"],
    "required_context": ["cache_ttl", "cache_miss_handling", "key_distribution"],
    "trigger_conditions": [
      "many clients request same key simultaneously",
      "cache misses propagate to backend",
      "no_ttl_randomization",
      "backend latency spikes under load"
    ],
    "safety_signals": [
      "ttl_jitter",
      "cache_warming",
      "hot_key_sharding",
      "rate_limiting_on_misses",
      "semaphore_on_backend_calls"
    ],
    "why_subtle": [
      "staging rarely has traffic skew",
      "p50 latency stable until threshold crossed"
    ],
    "discussion_questions": [
      "Is caching TTL randomized to prevent thundering herd?",
      "Are hot keys sharded or rate-limited?",
      "Is fallback to backend controlled with semaphores?",
      "What happens when cache cluster fails?"
    ]
  },
  {
    "id": "consistency_drift",
    "name": "Consistency Drift / Stale Reads",
    "signals": ["replication", "read-after-write", "latency"],
    "required_context": ["consistency_model", "replication_lag", "read_preference"],
    "trigger_conditions": [
      "replication_lag > X ms",
      "read-after-write expectation not met",
      "writes occur concurrently on multiple regions",
      "eventual_consistency_without_compensation"
    ],
    "safety_signals": [
      "strong_consistency_for_critical_paths",
      "version_vectors",
      "causal_consistency",
      "read_repair",
      "stale_read_detection"
    ],
    "why_subtle": [
      "often only visible under real production load",
      "staging may not replicate multi-region behavior"
    ],
    "discussion_questions": [
      "Is strong consistency required or eventual is acceptable?",
      "Are read and write paths properly coordinated?",
      "Are caches invalidated correctly?",
      "What is the maximum acceptable replication lag?"
    ]
  },
  {
    "id": "thundering_herd",
    "name": "Thundering Herd",
    "signals": ["fanout", "retry", "simultaneous"],
    "required_context": ["request_timing", "resource_availability", "scaling_policy"],
    "trigger_conditions": [
      "many clients request resource simultaneously",
      "backend service cannot scale fast enough",
      "no_caching_or_rate_limiting",
      "synchronous_resource_initialization"
    ],
    "safety_signals": [
      "rate_limiting",
      "exponential_backoff",
      "request_staggering",
      "warmup_procedures",
      "circuit_breaker"
    ],
    "why_subtle": [
      "p50 latency looks normal",
      "rare in test environments",
      "often triggered only at peak usage"
    ],
    "discussion_questions": [
      "Is rate limiting in place?",
      "Are backoff strategies implemented?",
      "Are requests staggered when possible?",
      "Can the backend handle sudden traffic spikes?"
    ]
  },
  {
    "id": "tail_latency_amplification",
    "name": "Tail Latency Amplification",
    "signals": ["p99_latency", "fanout", "dependency"],
    "required_context": ["p99_slo", "dependency_latency_distribution", "fanout_depth"],
    "trigger_conditions": [
      "single slow dependency in critical path",
      "fanout > 1",
      "no_hedging_or_fallback",
      "p99_slo_violation_under_load"
    ],
    "safety_signals": [
      "latency_hedging",
      "timeout_propagation",
      "slow_dependency_isolation",
      "retry_budget_for_tail_spikes",
      "cancellation_on_timeout"
    ],
    "why_subtle": [
      "p50 and p95 look fine",
      "hard to detect until production scale"
    ],
    "discussion_questions": [
      "Is latency hedging applied?",
      "Are slow dependencies isolated?",
      "Is there retry budget for tail spikes?",
      "Are timeouts properly propagated end-to-end?"
    ]
  },
  {
    "id": "partition_hotspot",
    "name": "Partition Hotspot / Skew",
    "signals": ["partition", "load", "latency"],
    "required_context": ["partitioning_strategy", "key_distribution", "rebalancing_policy"],
    "trigger_conditions": [
      "hash_skew_causes_uneven_load",
      "single node receives disproportionate traffic",
      "no_dynamic_rebalancing",
      "hot_keys_not_handled"
    ],
    "safety_signals": [
      "automatic_rebalancing",
      "hot_key_detection",
      "key_sharding",
      "load_monitoring_per_partition",
      "adaptive_partitioning"
    ],
    "why_subtle": [
      "average latency appears normal",
      "staging often has smaller dataset and uniform key distribution"
    ],
    "discussion_questions": [
      "Are partitions evenly balanced?",
      "Is there automatic rebalancing?",
      "Are hot keys rate-limited or sharded?",
      "How is partition health monitored?"
    ]
  },
  {
    "id": "retry_budget_exceeded",
    "name": "Retry Budget Exceeded / Cascading Failures",
    "signals": ["retry", "dependency", "timeout"],
    "required_context": ["retry_policy", "circuit_breaker_config", "dependency_count"],
    "trigger_conditions": [
      "retries propagate to multiple dependencies",
      "no_circuit_breakers",
      "high_load_scenario",
      "retry_budget_exceeded"
    ],
    "safety_signals": [
      "circuit_breakers",
      "retry_budget_per_service",
      "exponential_backoff_with_jitter",
      "cascading_failure_prevention",
      "dependency_isolation"
    ],
    "why_subtle": [
      "fails silently until retries cascade",
      "p50 latency may still appear healthy"
    ],
    "discussion_questions": [
      "Are circuit breakers implemented?",
      "Is retry budget defined per service?",
      "Are retries exponential with jitter?",
      "How are cascading failures prevented?"
    ]
  },
  {
    "id": "network_partition",
    "name": "Network Partition / Split Brain",
    "signals": ["network", "latency", "consistency"],
    "required_context": ["network_topology", "consistency_model", "leader_election"],
    "trigger_conditions": [
      "network_segment_unavailable",
      "replication_continues_in_isolated_cluster",
      "consistency_assumptions_violated",
      "split_brain_scenario"
    ],
    "safety_signals": [
      "quorum_checks",
      "leader_election_robustness",
      "partition_recovery_procedures",
      "consensus_protocol",
      "network_partition_detection"
    ],
    "why_subtle": [
      "staging rarely simulates network partitions",
      "issues emerge only under partial connectivity"
    ],
    "discussion_questions": [
      "Is leader election robust?",
      "Are quorum checks in place?",
      "Is partition recovery well-tested?",
      "How does the system handle split-brain scenarios?"
    ]
  },
  {
    "id": "resource_exhaustion",
    "name": "Resource Exhaustion / OOM / File Descriptor Limits",
    "signals": ["cpu", "memory", "fd", "latency", "crash"],
    "required_context": ["resource_limits", "scaling_policy", "graceful_degradation"],
    "trigger_conditions": [
      "unbounded_request_growth",
      "memory_or_fd_limits_exceeded",
      "no_graceful_degradation",
      "resource_leaks"
    ],
    "safety_signals": [
      "resource_limits_defined",
      "autoscaling_enabled",
      "graceful_degradation_strategies",
      "resource_monitoring",
      "circuit_breaker_on_resource_pressure"
    ],
    "why_subtle": [
      "service may appear healthy under normal load",
      "fails under peak load or unexpected bursts"
    ],
    "discussion_questions": [
      "Are resource limits defined?",
      "Is autoscaling enabled?",
      "Are graceful degradation strategies in place?",
      "How are resource leaks detected and prevented?"
    ]
  },
  {
    "id": "clock_skew",
    "name": "Clock Skew / Time Synchronization",
    "signals": ["timestamp", "ordering", "ttl", "expiration"],
    "required_context": ["clock_source", "consistency_model"],
    "trigger_conditions": [
      "lww_resolution (Last Write Wins)",
      "distributed_lock_ttl",
      "cross_node_event_ordering",
      "time_based_cache_invalidation"
    ],
    "safety_signals": [
      "logical_clocks (Lamport/Vector)",
      "hybrid_logical_clocks",
      "true_time_api",
      "causal_consistency",
      "ntp_synchronization"
    ],
    "why_subtle": [
      "drift accumulates slowly over weeks",
      "rarely manifests on single-node development environments"
    ],
    "discussion_questions": [
      "Does the system rely on synchronized wall-clock time for correctness?",
      "How does the system handle a 500ms drift between nodes?",
      "Are you using sequence numbers instead of timestamps for ordering?",
      "Is NTP synchronization enforced across all nodes?"
    ]
  },
  {
    "id": "idempotency_violations",
    "name": "Idempotency Violations / Duplicate Operations",
    "signals": ["retry", "duplicate", "idempotency", "transaction", "deduplication"],
    "required_context": ["idempotency_strategy", "retry_policy", "transaction_isolation"],
    "trigger_conditions": [
      "operations are not idempotent",
      "retries cause duplicate side effects",
      "no_idempotency_keys_or_deduplication",
      "state_changes_occur_on_retry"
    ],
    "safety_signals": [
      "idempotency_keys",
      "deduplication_logic",
      "transaction_isolation",
      "idempotency_check_before_state_change",
      "idempotency_window_management"
    ],
    "why_subtle": [
      "single request works correctly",
      "only manifests when network issues trigger retries",
      "staging rarely simulates retry scenarios"
    ],
    "discussion_questions": [
      "Are all operations idempotent?",
      "Are idempotency keys used for state-changing operations?",
      "Is there deduplication logic for duplicate requests?",
      "Are database transactions properly isolated?"
    ]
  },
  {
    "id": "connection_pool_exhaustion",
    "name": "Database Connection Pool Exhaustion",
    "signals": ["database", "connection", "pool", "timeout", "wait"],
    "required_context": ["pool_size", "connection_lifetime", "query_timeout"],
    "trigger_conditions": [
      "connection_pool_size_too_small",
      "connections_not_released_properly",
      "long_running_queries_hold_connections",
      "connection_leaks_in_error_paths"
    ],
    "safety_signals": [
      "appropriate_pool_sizing",
      "connection_timeout",
      "query_timeout",
      "connection_leak_detection",
      "separate_pools_for_long_queries"
    ],
    "why_subtle": [
      "works fine under normal load",
      "pool exhaustion causes cascading timeouts",
      "staging often has smaller connection pools that mask issues"
    ],
    "discussion_questions": [
      "Is connection pool size appropriate for expected load?",
      "Are connections always released, even in error cases?",
      "Are long-running queries isolated from connection pool?",
      "Is connection pool usage monitored and alerted?"
    ]
  },
  {
    "id": "service_discovery_staleness",
    "name": "Service Discovery Staleness / Registry Lag",
    "signals": ["service_discovery", "registry", "dns", "routing", "endpoint"],
    "required_context": ["discovery_ttl", "health_check_interval", "deregistration_delay"],
    "trigger_conditions": [
      "service_registry_updates_are_delayed",
      "dns_ttl_too_long_or_caching_too_aggressive",
      "load_balancer_uses_stale_endpoint_list",
      "service_instances_not_properly_deregistered"
    ],
    "safety_signals": [
      "short_discovery_ttl",
      "active_health_checking",
      "quick_deregistration",
      "stale_endpoint_detection",
      "circuit_breaker_on_stale_endpoints"
    ],
    "why_subtle": [
      "works correctly when services are stable",
      "only appears during deployments or failures",
      "staging rarely has rapid service churn"
    ],
    "discussion_questions": [
      "What is the TTL for service discovery updates?",
      "Are failed instances quickly removed from routing?",
      "Is there health checking to detect stale endpoints?",
      "How long does it take for new instances to be discoverable?"
    ]
  },
  {
    "id": "graceful_degradation_failure",
    "name": "Graceful Degradation / Fallback Mechanism Failures",
    "signals": ["fallback", "degradation", "circuit_breaker", "timeout", "error"],
    "required_context": ["fallback_strategy", "degraded_mode_capabilities", "fallback_testing"],
    "trigger_conditions": [
      "fallback_mechanisms_not_properly_tested",
      "fallback_itself_fails_or_times_out",
      "cascading_fallbacks_create_dependency_chains",
      "degraded_mode_not_actually_graceful"
    ],
    "safety_signals": [
      "fallback_testing_under_load",
      "fallback_timeout_limits",
      "circuit_breaker_on_fallback",
      "degraded_mode_validation",
      "fallback_monitoring"
    ],
    "why_subtle": [
      "primary path works fine in normal operation",
      "fallback only tested in isolation, not under load",
      "degraded mode may be worse than failure"
    ],
    "discussion_questions": [
      "Are fallback mechanisms tested under production-like conditions?",
      "Can fallbacks themselves fail or timeout?",
      "Is degraded mode actually usable, or does it just fail differently?",
      "Are there circuit breakers to prevent fallback cascades?"
    ]
  },
  {
    "id": "distributed_lock_contention",
    "name": "Distributed Lock Contention / Deadlock",
    "signals": ["lock", "mutex", "deadlock", "contention", "wait", "timeout"],
    "required_context": ["lock_ttl", "lock_acquisition_order", "contention_level"],
    "trigger_conditions": [
      "distributed_locks_held_for_too_long",
      "lock_acquisition_order_differs_across_services",
      "locks_not_released_on_failure",
      "high_contention_on_shared_resources"
    ],
    "safety_signals": [
      "lock_timeout",
      "consistent_acquisition_order",
      "automatic_lock_release",
      "deadlock_detection",
      "lock_contention_monitoring"
    ],
    "why_subtle": [
      "low contention works fine",
      "deadlocks only appear under high load",
      "staging rarely has concurrent access patterns"
    ],
    "discussion_questions": [
      "What is the maximum lock hold time?",
      "Is lock acquisition order consistent across services?",
      "Are locks always released, even on errors?",
      "Is there timeout and deadlock detection for locks?"
    ]
  },
  {
    "id": "event_ordering_violations",
    "name": "Event Ordering Violations / Out-of-Order Processing",
    "signals": ["event", "ordering", "sequence", "message", "queue", "stream"],
    "required_context": ["ordering_requirements", "partition_strategy", "sequence_numbering"],
    "trigger_conditions": [
      "events_processed_out_of_order",
      "multiple_partitions_or_queues",
      "no_sequence_numbers_or_ordering_guarantees",
      "parallel_processing_breaks_causality"
    ],
    "safety_signals": [
      "sequence_numbering",
      "partition_key_strategy",
      "causal_ordering",
      "out_of_order_detection",
      "reordering_buffer"
    ],
    "why_subtle": [
      "works correctly with single partition or low concurrency",
      "ordering issues only appear at scale",
      "staging often uses single-threaded processing"
    ],
    "discussion_questions": [
      "Are events required to be processed in order?",
      "Is there sequence numbering or versioning?",
      "How are ordering guarantees maintained across partitions?",
      "What happens if events arrive out of order?"
    ]
  },
  {
    "id": "configuration_drift",
    "name": "Configuration Drift / Environment Mismatch",
    "signals": ["config", "environment", "deployment", "version", "mismatch"],
    "required_context": ["config_versioning", "deployment_process", "environment_validation"],
    "trigger_conditions": [
      "configs_differ_between_environments",
      "config_changes_not_propagated_to_all_instances",
      "stale_configuration_cached",
      "feature_flags_or_toggles_inconsistent"
    ],
    "safety_signals": [
      "config_versioning",
      "config_validation",
      "config_audit_trail",
      "feature_flag_consistency_checks",
      "config_drift_detection"
    ],
    "why_subtle": [
      "works correctly in tested environment",
      "production config may differ from staging",
      "config changes may not be immediately visible"
    ],
    "discussion_questions": [
      "Are configurations versioned and audited?",
      "How are config changes propagated to all instances?",
      "Is there validation to detect config drift?",
      "Are feature flags consistent across services?"
    ]
  }
]
